# -*- coding: utf-8 -*-
"""GestoAI: An Italian Gesture Recognition Model using YOLOv5 for Real-Time Object Detection with Computer Vision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lUt1E2TxC-aXHEdqfooKcOXgr5IjcQrm

# **GestoAI: An Italian Gesture Recognition Model using YOLOv5 for Real-Time Object Detection with Computer Vision**


**About the Project**

GestoAI is an object detection model designed to recognize and interpret Italian hand gestures using the YOLOv5 architecture. This project aims to capture the richness of Italian nonverbal communication by identifying and classifying distinct hand gestures in real-time. By leveraging advanced computer vision, GestoAI has potential applications in cross-cultural communication, interactive education, and immersive experiences, bringing the expressive nature of Italian gestures into digital spaces.

**Project Owner: Manuel Contreras**

**1 - Settings**

The **YOLO (You Only Look Once) models** by **Ultralytics** are a family of real-time object detection models known for their speed and accuracy.

Some notable features include:

* Ease of Use: YOLOv5 provides a Pythonic interface with compatibility for PyTorch, making it very accessible for developers.
* Flexible Model Sizes: The different sizes are designed to fit various needs—smaller models like YOLOv5s are faster and suitable for real-time applications, while larger ones like YOLOv5x provide higher accuracy.
* Pre-trained Weights: Ultralytics offers pre-trained weights on popular datasets like COCO, enabling transfer learning and rapid deployment.
"""

# Commented out IPython magic to ensure Python compatibility.
# clone YOLOv5 repository
!git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5

# Commented out IPython magic to ensure Python compatibility.
# %pwd

# install dependencies
!pip install -qr requirements.txt
import torch

from IPython.display import Image, clear_output  # image displaying
# from utils.google_utils import gdrive_download  # to download models/datasets

# clear_output()
print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

"""**2- Import Dataset**

The dataset comprises images initially captured using a Jupyter-based algorithm and subsequently labeled through the Roboflow platform. It includes 240 images representing 17 distinct Italian hand gestures, specifically: Awesome, Be Silent, Calm Down, Delicious, Cornutto, Hello, I Cannot Stand You, I Did Not Hear Well, I Don’t Know, I Forgot, I’ve Had Enough, I Hope So, Let’s Go, Oh Please, Pay Attention to That, and This Person Is Such a Talker.
"""

# Commented out IPython magic to ensure Python compatibility.
# Export code snippet and paste here
# %cd /content
!curl -L "https://github.com/Manuel17410/Pics/raw/refs/heads/main/Italian%20Gestures.v1i.yolov5pytorch.zip" > SignData.zip; unzip SignData.zip; rm SignData.zip

"""**3- YAML file and its addaptation**"""

# Commented out IPython magic to ensure Python compatibility.
# this is the YAML file  we're loading into this notebook with our data
# %cat data.yaml

# Rewrite data.yaml without the val entry
data_yaml = """
train: ../train/images
test: ../test/images

nc: 17
names: ['Awesome', 'Be silent', 'Calm down', 'Cornutto', 'Delicious', 'Hello', 'I cannot stand you', 'I did not hear well', 'I do not care', 'I do not know', 'I forgot', 'I had enough', 'I hope so', 'Lets go', 'Oh please', 'Pay attention to that', 'This person is such a talker']

roboflow:
  workspace: manuel-contreras
  project: italian-gestures
  version: 1
  license: CC BY 4.0
  url: https://universe.roboflow.com/manuel-contreras/italian-gestures/dataset/1
"""

# Save the updated content to data.yaml
with open("/content/data.yaml", "w") as file:
    file.write(data_yaml)

print("Updated data.yaml without the val entry.")

# Commented out IPython magic to ensure Python compatibility.
# this is the YAML file  we're loading into this notebook with our data
# %cat data.yaml

#define number of classes based on YAML
import yaml
with open("data.yaml", 'r') as stream:
    num_classes = str(yaml.safe_load(stream)['nc'])

num_classes

"""**4- Model Configuration**"""

# Commented out IPython magic to ensure Python compatibility.
#this is the model configuration we will use
# %cat /content/yolov5/models/yolov5s.yaml

#customize iPython writefile so we can write variables
from IPython.core.magic import register_line_cell_magic

@register_line_cell_magic
def writetemplate(line, cell):
    with open(line, 'w') as f:
        f.write(cell.format(**globals()))

# Commented out IPython magic to ensure Python compatibility.
# %%writetemplate /content/yolov5/models/custom_yolov5s.yaml
# 
# # parameters
# nc: {num_classes}  # number of classes
# depth_multiple: 0.33  # model depth multiple
# width_multiple: 0.50  # layer channel multiple
# 
# # anchors
# anchors:
#   - [10,13, 16,30, 33,23]  # P3/8
#   - [30,61, 62,45, 59,119]  # P4/16
#   - [116,90, 156,198, 373,326]  # P5/32
# 
# # YOLOv5 backbone
# backbone:
#   # [from, number, module, args]
#   [[-1, 1, Focus, [64, 3]],  # 0-P1/2
#    [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
#    [-1, 3, BottleneckCSP, [128]],
#    [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
#    [-1, 9, BottleneckCSP, [256]],
#    [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
#    [-1, 9, BottleneckCSP, [512]],
#    [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
#    [-1, 1, SPP, [1024, [5, 9, 13]]],
#    [-1, 3, BottleneckCSP, [1024, False]],  # 9
#   ]
# 
# # YOLOv5 head
# head:
#   [[-1, 1, Conv, [512, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [[-1, 6], 1, Concat, [1]],  # cat backbone P4
#    [-1, 3, BottleneckCSP, [512, False]],  # 13
# 
#    [-1, 1, Conv, [256, 1, 1]],
#    [-1, 1, nn.Upsample, [None, 2, 'nearest']],
#    [[-1, 4], 1, Concat, [1]],  # cat backbone P3
#    [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)
# 
#    [-1, 1, Conv, [256, 3, 2]],
#    [[-1, 14], 1, Concat, [1]],  # cat head P4
#    [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)
# 
#    [-1, 1, Conv, [512, 3, 2]],
#    [[-1, 10], 1, Concat, [1]],  # cat head P5
#    [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)
# 
#    [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
#   ]

# Verify that the train and test directories have been set up correctly
!ls /content/train/images
!ls /content/test/images

# Create the data.yaml file with the necessary content
yaml_content = """
train: /content/train/images
val: /content/test/images

nc: 17
names: ['Awesome', 'Be silent', 'Calm down', 'Cornutto', 'Delicious',
        'Hello', 'I cannot stand you', 'I did not hear well',
        'I do not care', 'I do not know', 'I forgot', 'I had enough',
        'I hope so', 'Lets go', 'Oh please',
        'Pay attention to that', 'This person is such a talker']
"""

# Write the content to data.yaml
with open('/content/data.yaml', 'w') as f:
    f.write(yaml_content)

# Check the contents of the data.yaml file
!cat /content/data.yaml

"""**Model Training**

1- with 100 epochs

2- with 500 epochs
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# %cd /content/yolov5/
# !python train.py --img 416 --batch 16 --epochs 100 --data '/content/data.yaml' --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_results --cache
#

# we can also output some older school graphs if the tensor board isn't working for whatever reason...
from utils.plots import plot_results  # plot results.txt as results.png
Image(filename='/content/yolov5/runs/train/yolov5s_results2/results.png', width=1000)  # view results.png

# first, display our ground truth data
print("GROUND TRUTH TRAINING DATA:")
Image(filename='/content/yolov5/runs/train/yolov5s_results2/val_batch0_labels.jpg', width=900)

"""As we can see from the pictures above, the model is predicting really well. The only picture that it is getting wrong is the fourth, which should only be labeled as "I did not hear"."""

# Commented out IPython magic to ensure Python compatibility.
# # train yolov5s on custom data for 100 epochs
# # time its performance
# %%time
# %cd /content/yolov5/
# !python train.py --img 416 --batch 16 --epochs 500 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights 'yolov5s.pt' --name yolov5s_results  --cache
#

"""This model achieves an accuracy of **0.6**, which is good given the relatively small size of the dataset and the numerous categories it encompasses."""

# we can also output some older school graphs if the tensor board isn't working for whatever reason...
from utils.plots import plot_results  # plot results.txt as results.png
Image(filename='/content/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png

# first, display our ground truth data
print("GROUND TRUTH TRAINING DATA:")
Image(filename='/content/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)

# Commented out IPython magic to ensure Python compatibility.
# trained weights are saved by default in our weights folder
# %ls runs/

# Commented out IPython magic to ensure Python compatibility.
# %ls runs/train/yolov5s_results/weights

# Commented out IPython magic to ensure Python compatibility.

# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!
# use the best weights!
# %cd /content/yolov5/
!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.5 --source ../test/images

